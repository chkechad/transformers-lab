
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://chkechad.github.io/transformers-lab/api/">
      
      
        <link rel="prev" href="..">
      
      
      
        
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.7.1">
    
    
      
        <title>API - Transformers</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.484c7ddc.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.ab4e12ef.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../assets/_mkdocstrings.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="deep-orange">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#api" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="Transformers" class="md-header__button md-logo" aria-label="Transformers" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Transformers
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              API
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="deep-orange"  aria-hidden="true"  type="radio" name="__palette" id="__palette_0">
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="deep-orange"  aria-label="Dark mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Dark mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href=".." class="md-tabs__link">
        
  
  
    
  
  Project

      </a>
    </li>
  

      
        
  
  
  
    
  
  
    <li class="md-tabs__item md-tabs__item--active">
      <a href="./" class="md-tabs__link">
        
  
  
    
  
  API

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Transformers" class="md-nav__button md-logo" aria-label="Transformers" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Transformers
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Project
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    
  
    API
  

    
  </span>
  
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    
  
    API
  

    
  </span>
  
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#transformers_lab.feed_forward" class="md-nav__link">
    <span class="md-ellipsis">
      
        feed_forward
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="feed_forward">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#transformers_lab.feed_forward.FeedForward" class="md-nav__link">
    <span class="md-ellipsis">
      
        FeedForward
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="FeedForward">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#transformers_lab.feed_forward.FeedForward.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      
        __init__
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="__init__">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#transformers_lab.feed_forward.FeedForward.__init__--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      
        Parameters
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transformers_lab.feed_forward.FeedForward.forward" class="md-nav__link">
    <span class="md-ellipsis">
      
        forward
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="forward">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#transformers_lab.feed_forward.FeedForward.forward--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      
        Parameters
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transformers_lab.feed_forward.FeedForward.forward--returns" class="md-nav__link">
    <span class="md-ellipsis">
      
        Returns:
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transformers_lab.feed_forward.FeedForward.relu" class="md-nav__link">
    <span class="md-ellipsis">
      
        relu
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transformers_lab.feed_forward.xavier_init" class="md-nav__link">
    <span class="md-ellipsis">
      
        xavier_init
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="xavier_init">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#transformers_lab.feed_forward.xavier_init--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      
        Parameters
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transformers_lab.feed_forward.xavier_init--returns" class="md-nav__link">
    <span class="md-ellipsis">
      
        Returns:
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#transformers_lab.layer_norm" class="md-nav__link">
    <span class="md-ellipsis">
      
        layer_norm
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="layer_norm">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#transformers_lab.layer_norm.LayerNorm" class="md-nav__link">
    <span class="md-ellipsis">
      
        LayerNorm
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="LayerNorm">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#transformers_lab.layer_norm.LayerNorm.__call__" class="md-nav__link">
    <span class="md-ellipsis">
      
        __call__
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="__call__">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#transformers_lab.layer_norm.LayerNorm.__call__--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      
        Parameters
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transformers_lab.layer_norm.LayerNorm.__call__--returns" class="md-nav__link">
    <span class="md-ellipsis">
      
        Returns:
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transformers_lab.layer_norm.LayerNorm.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      
        __init__
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="__init__">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#transformers_lab.layer_norm.LayerNorm.__init__--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      
        Parameters
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transformers_lab.layer_norm.LayerNorm.__init__--attributes" class="md-nav__link">
    <span class="md-ellipsis">
      
        Attributes:
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#transformers_lab.multihead_attention" class="md-nav__link">
    <span class="md-ellipsis">
      
        multihead_attention
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="multihead_attention">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#transformers_lab.multihead_attention.multi_head_attention" class="md-nav__link">
    <span class="md-ellipsis">
      
        multi_head_attention
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="multi_head_attention">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#transformers_lab.multihead_attention.multi_head_attention--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      
        Parameters
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transformers_lab.multihead_attention.multi_head_attention--returns" class="md-nav__link">
    <span class="md-ellipsis">
      
        Returns:
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transformers_lab.multihead_attention.scaled_dot_product_attention" class="md-nav__link">
    <span class="md-ellipsis">
      
        scaled_dot_product_attention
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="scaled_dot_product_attention">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#transformers_lab.multihead_attention.scaled_dot_product_attention--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      
        Parameters
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transformers_lab.multihead_attention.scaled_dot_product_attention--returns" class="md-nav__link">
    <span class="md-ellipsis">
      
        Returns:
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#transformers_lab.positional_encoding" class="md-nav__link">
    <span class="md-ellipsis">
      
        positional_encoding
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="positional_encoding">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#transformers_lab.positional_encoding.sinusoidal_positional_encoding" class="md-nav__link">
    <span class="md-ellipsis">
      
        sinusoidal_positional_encoding
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="sinusoidal_positional_encoding">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#transformers_lab.positional_encoding.sinusoidal_positional_encoding--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      
        Parameters
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transformers_lab.positional_encoding.sinusoidal_positional_encoding--returns" class="md-nav__link">
    <span class="md-ellipsis">
      
        Returns:
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#transformers_lab.self_attention" class="md-nav__link">
    <span class="md-ellipsis">
      
        self_attention
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="self_attention">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#transformers_lab.self_attention.scaled_dot_product_attention" class="md-nav__link">
    <span class="md-ellipsis">
      
        scaled_dot_product_attention
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="scaled_dot_product_attention">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#transformers_lab.self_attention.scaled_dot_product_attention--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      
        Parameters
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transformers_lab.self_attention.scaled_dot_product_attention--returns" class="md-nav__link">
    <span class="md-ellipsis">
      
        Returns:
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transformers_lab.self_attention.self_attention" class="md-nav__link">
    <span class="md-ellipsis">
      
        self_attention
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="self_attention">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#transformers_lab.self_attention.self_attention--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      
        Parameters
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transformers_lab.self_attention.self_attention--returns" class="md-nav__link">
    <span class="md-ellipsis">
      
        Returns:
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#transformers_lab.softmax" class="md-nav__link">
    <span class="md-ellipsis">
      
        softmax
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="softmax">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#transformers_lab.softmax.softmax" class="md-nav__link">
    <span class="md-ellipsis">
      
        softmax
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#transformers_lab.feed_forward" class="md-nav__link">
    <span class="md-ellipsis">
      
        feed_forward
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="feed_forward">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#transformers_lab.feed_forward.FeedForward" class="md-nav__link">
    <span class="md-ellipsis">
      
        FeedForward
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="FeedForward">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#transformers_lab.feed_forward.FeedForward.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      
        __init__
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="__init__">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#transformers_lab.feed_forward.FeedForward.__init__--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      
        Parameters
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transformers_lab.feed_forward.FeedForward.forward" class="md-nav__link">
    <span class="md-ellipsis">
      
        forward
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="forward">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#transformers_lab.feed_forward.FeedForward.forward--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      
        Parameters
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transformers_lab.feed_forward.FeedForward.forward--returns" class="md-nav__link">
    <span class="md-ellipsis">
      
        Returns:
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transformers_lab.feed_forward.FeedForward.relu" class="md-nav__link">
    <span class="md-ellipsis">
      
        relu
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transformers_lab.feed_forward.xavier_init" class="md-nav__link">
    <span class="md-ellipsis">
      
        xavier_init
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="xavier_init">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#transformers_lab.feed_forward.xavier_init--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      
        Parameters
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transformers_lab.feed_forward.xavier_init--returns" class="md-nav__link">
    <span class="md-ellipsis">
      
        Returns:
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#transformers_lab.layer_norm" class="md-nav__link">
    <span class="md-ellipsis">
      
        layer_norm
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="layer_norm">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#transformers_lab.layer_norm.LayerNorm" class="md-nav__link">
    <span class="md-ellipsis">
      
        LayerNorm
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="LayerNorm">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#transformers_lab.layer_norm.LayerNorm.__call__" class="md-nav__link">
    <span class="md-ellipsis">
      
        __call__
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="__call__">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#transformers_lab.layer_norm.LayerNorm.__call__--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      
        Parameters
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transformers_lab.layer_norm.LayerNorm.__call__--returns" class="md-nav__link">
    <span class="md-ellipsis">
      
        Returns:
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transformers_lab.layer_norm.LayerNorm.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      
        __init__
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="__init__">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#transformers_lab.layer_norm.LayerNorm.__init__--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      
        Parameters
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transformers_lab.layer_norm.LayerNorm.__init__--attributes" class="md-nav__link">
    <span class="md-ellipsis">
      
        Attributes:
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#transformers_lab.multihead_attention" class="md-nav__link">
    <span class="md-ellipsis">
      
        multihead_attention
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="multihead_attention">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#transformers_lab.multihead_attention.multi_head_attention" class="md-nav__link">
    <span class="md-ellipsis">
      
        multi_head_attention
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="multi_head_attention">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#transformers_lab.multihead_attention.multi_head_attention--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      
        Parameters
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transformers_lab.multihead_attention.multi_head_attention--returns" class="md-nav__link">
    <span class="md-ellipsis">
      
        Returns:
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transformers_lab.multihead_attention.scaled_dot_product_attention" class="md-nav__link">
    <span class="md-ellipsis">
      
        scaled_dot_product_attention
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="scaled_dot_product_attention">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#transformers_lab.multihead_attention.scaled_dot_product_attention--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      
        Parameters
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transformers_lab.multihead_attention.scaled_dot_product_attention--returns" class="md-nav__link">
    <span class="md-ellipsis">
      
        Returns:
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#transformers_lab.positional_encoding" class="md-nav__link">
    <span class="md-ellipsis">
      
        positional_encoding
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="positional_encoding">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#transformers_lab.positional_encoding.sinusoidal_positional_encoding" class="md-nav__link">
    <span class="md-ellipsis">
      
        sinusoidal_positional_encoding
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="sinusoidal_positional_encoding">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#transformers_lab.positional_encoding.sinusoidal_positional_encoding--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      
        Parameters
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transformers_lab.positional_encoding.sinusoidal_positional_encoding--returns" class="md-nav__link">
    <span class="md-ellipsis">
      
        Returns:
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#transformers_lab.self_attention" class="md-nav__link">
    <span class="md-ellipsis">
      
        self_attention
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="self_attention">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#transformers_lab.self_attention.scaled_dot_product_attention" class="md-nav__link">
    <span class="md-ellipsis">
      
        scaled_dot_product_attention
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="scaled_dot_product_attention">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#transformers_lab.self_attention.scaled_dot_product_attention--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      
        Parameters
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transformers_lab.self_attention.scaled_dot_product_attention--returns" class="md-nav__link">
    <span class="md-ellipsis">
      
        Returns:
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transformers_lab.self_attention.self_attention" class="md-nav__link">
    <span class="md-ellipsis">
      
        self_attention
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="self_attention">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#transformers_lab.self_attention.self_attention--parameters" class="md-nav__link">
    <span class="md-ellipsis">
      
        Parameters
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transformers_lab.self_attention.self_attention--returns" class="md-nav__link">
    <span class="md-ellipsis">
      
        Returns:
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#transformers_lab.softmax" class="md-nav__link">
    <span class="md-ellipsis">
      
        softmax
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="softmax">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#transformers_lab.softmax.softmax" class="md-nav__link">
    <span class="md-ellipsis">
      
        softmax
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="api">API</h1>


<div class="doc doc-object doc-module">



<h2 id="transformers_lab.feed_forward" class="doc doc-heading">
            <code>transformers_lab.feed_forward</code>


</h2>

    <div class="doc doc-contents first">

        <p>Feed forward neural network module.</p>










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h3 id="transformers_lab.feed_forward.FeedForward" class="doc doc-heading">
            <code>FeedForward</code>


</h3>


    <div class="doc doc-contents ">



        <p>Position-wise Feed Forward Network used in Transformer.</p>











<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h4 id="transformers_lab.feed_forward.FeedForward.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">,</span> <span class="n">init_weight_fn</span><span class="o">=</span><span class="n">xavier_init</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Initialize the feed-forward module.</p>
<h6 id="transformers_lab.feed_forward.FeedForward.__init__--parameters">Parameters</h6>
<p>d_model : int
    Model dimensionality
d_ff : int
    Hidden layer dimensionality
init_weight_fn : Callable[[int, int], np.ndarray]
    Weight initialization function. Defaults to xavier_init.</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="transformers_lab.feed_forward.FeedForward.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Run forward pass.</p>
<h6 id="transformers_lab.feed_forward.FeedForward.forward--parameters">Parameters</h6>
<p>x : np.ndarray
    Shape (seq_len, d_model)</p>
<h6 id="transformers_lab.feed_forward.FeedForward.forward--returns">Returns:</h6>
<p>np.ndarray
    Shape (seq_len, d_model)</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="transformers_lab.feed_forward.FeedForward.relu" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>ReLU activation function.</p>


    </div>

</div>



  </div>

    </div>

</div>


<div class="doc doc-object doc-function">


<h3 id="transformers_lab.feed_forward.xavier_init" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">xavier_init</span><span class="p">(</span><span class="n">n1</span><span class="p">,</span> <span class="n">n2</span><span class="p">,</span> <span class="n">n_heads</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Xavier Glorot initialization for weight matrices.</p>
<h5 id="transformers_lab.feed_forward.xavier_init--parameters">Parameters</h5>
<p>n1 : int
    Input dimension.
n2 : int
    Output dimension.
n_heads : int, optional
    If provided, returns shape (n_heads, n1, n2).
    If None, returns shape (n1, n2).</p>
<h5 id="transformers_lab.feed_forward.xavier_init--returns">Returns:</h5>
<p>np.ndarray
    Shape (n_heads, n1, n2) or (n1, n2).</p>


    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="transformers_lab.layer_norm" class="doc doc-heading">
            <code>transformers_lab.layer_norm</code>


</h2>

    <div class="doc doc-contents first">

        <p>layer normalisation implementation.</p>










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h3 id="transformers_lab.layer_norm.LayerNorm" class="doc doc-heading">
            <code>LayerNorm</code>


</h3>


    <div class="doc doc-contents ">



        <p>Layer normalization implementation.</p>











<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h4 id="transformers_lab.layer_norm.LayerNorm.__call__" class="doc doc-heading">
            <code class="highlight language-python"><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Apply layer normalization.</p>
<h6 id="transformers_lab.layer_norm.LayerNorm.__call__--parameters">Parameters</h6>
<p>x : np.ndarray
    Input tensor of shape (..., d_model).</p>
<h6 id="transformers_lab.layer_norm.LayerNorm.__call__--returns">Returns:</h6>
<p>np.ndarray
    Normalized tensor with the same shape as input.</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="transformers_lab.layer_norm.LayerNorm.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-06</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Initialize the LayerNorm module.</p>
<h6 id="transformers_lab.layer_norm.LayerNorm.__init__--parameters">Parameters</h6>
<p>d_model : int
    Dimensionality of the input.
eps : float, optional
    Value added to the denominator for numerical stability.
    Default is 1e-6.</p>
<h6 id="transformers_lab.layer_norm.LayerNorm.__init__--attributes">Attributes:</h6>
<p>gamma : np.ndarray
    Scale parameter of shape (d_model,).
beta : np.ndarray
    Shift parameter of shape (d_model,).
eps : float
    Numerical stability constant.</p>


    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="transformers_lab.multihead_attention" class="doc doc-heading">
            <code>transformers_lab.multihead_attention</code>


</h2>

    <div class="doc doc-contents first">

        <p>Compute multi-head-attention.</p>










<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h3 id="transformers_lab.multihead_attention.multi_head_attention" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">multi_head_attention</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w_q</span><span class="p">,</span> <span class="n">w_k</span><span class="p">,</span> <span class="n">w_v</span><span class="p">,</span> <span class="n">w_o</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">x_cross</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Computes multi-head attention.</p>
<h5 id="transformers_lab.multihead_attention.multi_head_attention--parameters">Parameters</h5>
<p>x : np.ndarray
    Input tensor of shape (seq_len, d_model)</p>


<details class="w_q-" open>
  <summary>np.ndarray</summary>
  <p>Query weights of shape (n_heads, d_model, d_k)</p>
</details>

<details class="w_k-" open>
  <summary>np.ndarray</summary>
  <p>Key weights of shape (n_heads, d_model, d_k)</p>
</details>

<details class="w_v-" open>
  <summary>np.ndarray</summary>
  <p>Value weights of shape (n_heads, d_model, d_k)</p>
</details>

<details class="w_o-" open>
  <summary>np.ndarray</summary>
  <p>Output projection matrix of shape (d_model, d_model)</p>
</details>

<details class="n_heads-" open>
  <summary>int</summary>
  <p>Number of attention heads</p>
</details>

<details class="x_cross-" open>
  <summary>np.ndarray, optional</summary>
  <p>Encoder output of shape (src_seq_len, d_model).
If provided, keys and values come from x_cross (cross-attention).
If None, keys and values come from x (self-attention).</p>
</details>        <p>mask : np.ndarray, optional
    Mask of shape (seq_len, seq_len).
    Use make_causal_mask() for masked self-attention in the decoder.</p>
<h5 id="transformers_lab.multihead_attention.multi_head_attention--returns">Returns:</h5>
<p>np.ndarray
    Output tensor of shape (seq_len, d_model)</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="transformers_lab.multihead_attention.scaled_dot_product_attention" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">scaled_dot_product_attention</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Compute scaled dot-product attention.</p>
<h5 id="transformers_lab.multihead_attention.scaled_dot_product_attention--parameters">Parameters</h5>
<p>q : np.ndarray
    Query matrix of shape (seq_len, d_k)</p>


<details class="k-" open>
  <summary>np.ndarray</summary>
  <p>Key matrix of shape (seq_len, d_k)</p>
</details>

<details class="v-" open>
  <summary>np.ndarray</summary>
  <p>Value matrix of shape (seq_len, d_k)</p>
</details>        <p>mask: np.ndarray or None
    Mask to apply during attention computation. Must be of shape (seq_len, seq_len)</p>
<h5 id="transformers_lab.multihead_attention.scaled_dot_product_attention--returns">Returns:</h5>
<p>np.ndarray
    Output matrix of shape (seq_len, d_k)</p>


    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="transformers_lab.positional_encoding" class="doc doc-heading">
            <code>transformers_lab.positional_encoding</code>


</h2>

    <div class="doc doc-contents first">

        <p>Positional encoding.</p>










<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h3 id="transformers_lab.positional_encoding.sinusoidal_positional_encoding" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">sinusoidal_positional_encoding</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Compute sinusoidal positional encoding.</p>
<h5 id="transformers_lab.positional_encoding.sinusoidal_positional_encoding--parameters">Parameters</h5>
<p>seq_len : int
    Length of the input sequence</p>


<details class="d_model-" open>
  <summary>int</summary>
  <p>Dimensionality of the model</p>
</details>        <h5 id="transformers_lab.positional_encoding.sinusoidal_positional_encoding--returns">Returns:</h5>
<p>np.ndarray
    Positional encoding matrix of shape (seq_len, d_model)</p>


    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="transformers_lab.self_attention" class="doc doc-heading">
            <code>transformers_lab.self_attention</code>


</h2>

    <div class="doc doc-contents first">

        <p>Compute self-attention.</p>










<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h3 id="transformers_lab.self_attention.scaled_dot_product_attention" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">scaled_dot_product_attention</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Compute scaled dot-product attention.</p>
<h5 id="transformers_lab.self_attention.scaled_dot_product_attention--parameters">Parameters</h5>
<p>q : np.ndarray
    Query matrix of shape (seq_len, d_k)</p>


<details class="k-" open>
  <summary>np.ndarray</summary>
  <p>Key matrix of shape (seq_len, d_k)</p>
</details>

<details class="v-" open>
  <summary>np.ndarray</summary>
  <p>Value matrix of shape (seq_len, d_k)</p>
</details>        <p>mask: np.ndarray or None
    Mask to apply during attention computation. Must be of shape (seq_len, seq_len)</p>
<h5 id="transformers_lab.self_attention.scaled_dot_product_attention--returns">Returns:</h5>
<p>np.ndarray
    Output matrix of shape (seq_len, d_k)</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="transformers_lab.self_attention.self_attention" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">self_attention</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w_q</span><span class="p">,</span> <span class="n">w_k</span><span class="p">,</span> <span class="n">w_v</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Compute single-head self-attention.</p>
<h5 id="transformers_lab.self_attention.self_attention--parameters">Parameters</h5>
<p>x : np.ndarray
    Input embeddings of shape (seq_len, d_model)</p>


<details class="w_q-" open>
  <summary>np.ndarray</summary>
  <p>Query projection matrix of shape (d_model, d_k)</p>
</details>

<details class="w_k-" open>
  <summary>np.ndarray</summary>
  <p>Key projection matrix of shape (d_model, d_k)</p>
</details>

<details class="w_v-" open>
  <summary>np.ndarray</summary>
  <p>Value projection matrix of shape (d_model, d_k)</p>
</details>

<details class="mask-" open>
  <summary>np.ndarray, optional</summary>
  <p>Mask of shape (seq_len, seq_len). Defaults to None.</p>
</details>        <h5 id="transformers_lab.self_attention.self_attention--returns">Returns:</h5>
<p>np.ndarray
    Output of shape (seq_len, d_k)</p>


    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="transformers_lab.softmax" class="doc doc-heading">
            <code>transformers_lab.softmax</code>


</h2>

    <div class="doc doc-contents first">

        <p>Softmax is used to!</p>
<ul>
<li>transform arbitrary real-valued scores into probabilities</li>
<li>ensure all values are positive</li>
<li>ensure the probabilities sum to 1</li>
<li>normalize scores along a specified axis</li>
</ul>
<p>In Transformers, softmax is used to convert
similarity scores (attention scores) into attention weights.</p>










<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h3 id="transformers_lab.softmax.softmax" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Compute the softmax function in a numerically stable way.</p>
<p>The softmax function transforms arbitrary real-valued scores
into positive probabilities that sum to 1 along the specified axis.</p>
<p>In Transformers, it is commonly used to convert attention
similarity scores into attention weights.</p>
<p>:param x: NumPy array containing the input scores.
:param axis: Axis along which the normalization is applied.
             Defaults to the last axis.
:return: NumPy array of the same shape as x containing
         the normalized probabilities.</p>


    </div>

</div>



  </div>

    </div>

</div>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      
      <script id="__config" type="application/json">{"annotate": null, "base": "..", "features": ["navigation.instant", "navigation.tabs", "content.code.copy"], "search": "../assets/javascripts/workers/search.2c215733.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../assets/javascripts/bundle.79ae519e.min.js"></script>
      
    
  </body>
</html>