# Transformers

## Prerequisite

- Python3.12
- Numpy
- Algèbre linéaire
    - Vecteurs
    - Matrices
    - Produits matriciel
    - Softmax

## Pourquoi les transformers existe :

- RNN / LSTM
- Attention
- Attention is all you need

## Self attention

- Query,Key, Value
- Scaled Dot-Product Attention
- Softmax Stable
- Masque (Padding + Causal)

## Multi-Head attention

- Pourquoi plusieurs têtes
- Comment splitter / Concaténer
- Comment garder les dimensions propre

## Positional Encoding

- Sinus / Cosinus
- Encodage appris
- impact réel sur le modèle

## Bloc transformer

- attention
- residual connections
- layer normalization
- feed-forward network

## Encoder, Decoder, masques

- Encoder-only (BERT)
- Encoder-Decoder (T5)
- Encoder-only (GPT)

## Entraîner un mini-Transformer

- task simple (copy task, traduction toy)
- loss
- backprop (au moins conceptuellement)
- limitations du “Python pur”

